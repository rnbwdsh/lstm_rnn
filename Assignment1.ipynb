{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Numpy RNN\n",
    "Implement a RNN and run BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \"\"\"Numpy implementation of sequence-to-one recurrent neural network for regression tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        \"\"\"Initialization \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "            Number of input features per time step\n",
    "        hidden_size : int\n",
    "            Number of hidden units in the RNN\n",
    "        output_size : int\n",
    "            Number of output units.\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size   # D in literature\n",
    "        self.hidden_size = hidden_size # I in literature\n",
    "        self.output_size = output_size # K in literature\n",
    "\n",
    "        # create and initialize weights of the network\n",
    "        # as 90% of the usages in the scriptum are W.T, R.T, V.T\n",
    "        init = lambda shape: np.random.uniform(-0.2, 0.2, shape)\n",
    "        self.W = init((hidden_size, input_size)) # I X D\n",
    "        self.R = init((hidden_size, hidden_size)) # I x I\n",
    "        self.bs = np.zeros((hidden_size))\n",
    "        self.V = init((output_size, hidden_size)) # K x I\n",
    "        self.by = np.zeros((output_size))\n",
    "\n",
    "        # place holder to store intermediates for backprop\n",
    "        self.a = None\n",
    "        self.y_hat = None\n",
    "        self.grads = None\n",
    "        self.x = None\n",
    "\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass through the RNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Input sequence(s) of shape [sequence length, number of features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NumPy array containing the network prediction for the input sample.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        # as we have no activation function (f(t) is linear)\n",
    "        # a(t) = f(s(t)) = s(t) = W^T . x(t) + R^T . a(t-1) + bs\n",
    "        # = tanh( W^T . x(t) + R^T . a(t-1) + bs )\n",
    "        self.a = np.zeros((self.input_size, self.hidden_size)) # to make accessing t = -1 possible\n",
    "        for t in range(len(x)):\n",
    "            self.a[t] = np.tanh(self.W @ x[t] + self.R @ self.a[t-1] + self.bs)\n",
    "        self.y_hat = self.V @ self.a[t] + self.by\n",
    "        return self.y_hat  # sequence-to-1 model, so we only return the last \n",
    "\n",
    "    \n",
    "    def forward_fast(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" optimized method without saving to self.a \"\"\"\n",
    "        a = np.tanh(self.W @ x[0] + self.bs)\n",
    "        for t in range(1, len(x)):\n",
    "            a = np.tanh(self.W @ x[t] + self.R @ a + self.bs)\n",
    "        return self.V @ a + self.by\n",
    "\n",
    "    \n",
    "    def backward(self, d_loss: np.ndarray) -> Dict:\n",
    "        \"\"\"Calculate the backward pass through the RNN.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        d_loss : np.ndarray\n",
    "            The gradient of the loss w.r.t the network output in the shape [output_size,]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dictionary containing the gradients for each network weight as key-value pair.\n",
    "        \"\"\"\n",
    "        # create view, so that we don't have to reshape every time we call it\n",
    "        a = self.a.reshape(self.a.shape[0], 1, self.a.shape[1])\n",
    "        x = self.x.reshape(self.x.shape[0], 1, self.x.shape[1])\n",
    "        \n",
    "        # needs to be calculated only once\n",
    "        d_V = d_loss @ a[-1]\n",
    "        d_by = d_loss\n",
    "        \n",
    "        # init with 0 and sum it up\n",
    "        d_W = np.zeros_like(self.W)\n",
    "        d_R = np.zeros_like(self.R)\n",
    "        d_bs = np.zeros_like(self.bs)\n",
    "        \n",
    "        # instead of using * diag, we use elementwise multiplication\n",
    "        delta = d_loss.T @ self.V * (1 - a[-1] ** 2)\n",
    "        \n",
    "        for t in reversed(range(self.input_size)):\n",
    "            d_bs += delta.reshape(self.bs.shape)\n",
    "            d_W += delta.T @ x[t]\n",
    "            if t > 0:\n",
    "                d_R += delta.T @ a[t-1]\n",
    "                # a[t] = tanh(..) -> derivation = 1-tanh² -> reuse already calculated tanh\n",
    "                # calculate delta for the next step at t-1\n",
    "                delta = delta @ self.R * (1 - a[t-1] ** 2)\n",
    "        \n",
    "        self.grads = {'W': d_W, 'R': d_R, 'V': d_V, 'bs': d_bs, 'by': d_by}\n",
    "        return self.grads\n",
    "    \n",
    "        \n",
    "    def update(self, lr: float):\n",
    "        # update weights, aggregation is already done in backward\n",
    "        w = self.get_weights()\n",
    "        for name in w.keys():\n",
    "            w[name] -= lr * self.grads[name]\n",
    "        \n",
    "        # reset internal class attributes\n",
    "        self.grads = {}\n",
    "        self.y_hat, self.a = None, None\n",
    "        \n",
    "    def get_weights(self) -> Dict:\n",
    "        return {'W': self.W, 'R': self.R, 'V': self.V, 'bs': self.bs, 'by': self.by}\n",
    "    \n",
    "    def set_weights(self, weights: Dict):\n",
    "        if not all(name in weights.keys() for name in ['W', 'R', 'V']):\n",
    "            raise ValueError(\"Missing one of 'W', 'R', 'V' keys in the weight dictionary\")\n",
    "        for name, w in weights.items():\n",
    "            self.__dir__[\"name\"] = w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Numerical gradient check</h2>\n",
    "\n",
    "To validate your implementation, especially the backward pass, use the two-sided gradient approximation given by the equation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_gradient(model: RNN, x: np.ndarray, eps: float=1e-7) -> Dict:\n",
    "    \"\"\"Implementation of the two-sided numerical gradient approximation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : RNN\n",
    "        The RNN model object\n",
    "    x : np.ndarray\n",
    "        Input sequence(s) of shape [sequence length, number of features]\n",
    "    eps : float\n",
    "        The epsilon used for numerical gradient approximation\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary containing the numerical gradients for each weight of the RNN. Make sure\n",
    "    to name the dictionary keys like the names of the RNN gradients dictionary (e.g. \n",
    "    'd_R' for the weight 'R')\n",
    "    \"\"\"\n",
    "    g = {}\n",
    "    # iterate all weight-matrices w and all positions i, and calculate the num. grad.\n",
    "    for name, w in model.get_weights().items():\n",
    "        # initialize weight gradients with zero\n",
    "        wg = np.zeros_like(w)\n",
    "        # this makes a backup copy of original weights\n",
    "        for i, orig in np.ndenumerate(w): # can be 1d or 2d\n",
    "            # caculate for +eps\n",
    "            w[i] += eps\n",
    "            plus = model.forward_fast(x)\n",
    "\n",
    "            # calculate for -eps\n",
    "            w[i] = orig - eps\n",
    "            minus = model.forward_fast(x)\n",
    "\n",
    "            w[i] = orig # reset\n",
    "            # set weight gradient for this weight and this index\n",
    "            wg[i] = np.sum(plus - minus) / (2*eps)\n",
    "        # add calculated weights into return-weights\n",
    "        g[name] = wg\n",
    "    return g\n",
    "\n",
    "\n",
    "def get_analytical_gradient(model: RNN, x: np.ndarray) -> Dict:\n",
    "    \"\"\"Helper function to get the analytical gradient.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : RNN\n",
    "        The RNN model object\n",
    "    x : np.ndarray\n",
    "        Input sequence(s) of shape [sequence length, number of features]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary containing the analytical gradients for each weight of the RNN.\n",
    "    \"\"\"\n",
    "    loss = model.forward(x)\n",
    "    return model.backward(np.ones((model.output_size, 1)))\n",
    "\n",
    "            \n",
    "def gradient_check(model: RNN, x: np.ndarray, treshold: float = 1e-7):\n",
    "    \"\"\"Perform gradient checking.\n",
    "    \n",
    "    You don't have to do anything in this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : RNN\n",
    "        The RNN model object\n",
    "    x : np.ndarray\n",
    "        Input sequence(s) of shape [sequence length, number of features]\n",
    "    eps : float\n",
    "        The epsilon used for numerical gradient approximation    \n",
    "    \"\"\"\n",
    "    numerical_grads = get_numerical_gradient(model, x)\n",
    "    analytical_grads = get_analytical_gradient(model, x)\n",
    "    \n",
    "    for key, num_grad in numerical_grads.items():\n",
    "        difference = np.linalg.norm(num_grad - analytical_grads[key])\n",
    "        # assert num_grad.shape == analytical_grads[key].shape\n",
    "        \n",
    "        if difference < treshold:\n",
    "            print(f\"Gradient check for {key} passed (difference {difference:.3e})\")\n",
    "        else:\n",
    "            print(f\"Gradient check for {key} failed (difference {difference:.3e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Compare the time for gradient computation</h2>\n",
    "Finally, use the code below to investigate the benefit of being able to calculate the exact analytical gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check with a single output neuron:\n",
      "Gradient check for W passed (difference 4.371e-10)\n",
      "Gradient check for R passed (difference 4.748e-10)\n",
      "Gradient check for V passed (difference 2.930e-11)\n",
      "Gradient check for bs passed (difference 1.721e-10)\n",
      "Gradient check for by passed (difference 5.939e-12)\n",
      "\n",
      "Gradient check with multiple output neurons:\n",
      "Gradient check for W passed (difference 5.764e-10)\n",
      "Gradient check for R passed (difference 7.208e-10)\n",
      "Gradient check for V passed (difference 1.329e-10)\n",
      "Gradient check for bs passed (difference 2.681e-10)\n",
      "Gradient check for by passed (difference 1.123e-10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient check with a single output neuron:\")\n",
    "model = RNN(input_size=5, hidden_size=10, output_size=1)\n",
    "x = np.random.rand(5, 5)\n",
    "gradient_check(model, x)\n",
    "\n",
    "print(\"\\nGradient check with multiple output neurons:\")\n",
    "model = RNN(input_size=5, hidden_size=10, output_size=5)\n",
    "x = np.random.rand(5, 5)\n",
    "gradient_check(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 µs ± 3.91 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "analytical_time = %timeit -o get_analytical_gradient(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1 ms ± 328 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "numerical_time = %timeit -o get_numerical_gradient(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The analytical gradient computation was 96 times faster\n"
     ]
    }
   ],
   "source": [
    "if analytical_time.average < numerical_time.average:\n",
    "    fraction = numerical_time.average / analytical_time.average\n",
    "    print(f\"The analytical gradient computation was {fraction:.0f} times faster\")\n",
    "else:\n",
    "    fraction = analytical_time.average / numerical_time.average\n",
    "    print(f\"The numerical gradient computation was {fraction:.0f} times faster\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
